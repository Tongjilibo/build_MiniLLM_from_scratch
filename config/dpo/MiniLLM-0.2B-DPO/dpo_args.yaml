dpo:
  one_dataset_every_time: true  # false表示一次把所有dpo数据吃进内存，true表示一次只使用一个dataset
  lr: 0.00001  # 学习率
  batch_size: 2  # 每张卡的batch size
  grad_accumulation_steps: 4  # 梯度累计
  pad_token_id: 0  # pad token id
  max_length: 1024   # 序列最大长度
  epochs: 1  # 训练的epoch数
  weight_decay: 0.05  # 权重衰减
  interval: 2000  # 保存权重的步数
  torch_dtype: null  # 默认使用混合精度训练，可以制定为torch.float32，torch.float16或者torch.bfloat16
  model_path: "../ckpt/sft/MiniLLM-0.2B-SFT/final_2.1458/model.pt"  # 预训练模型权重路径
  config_path: "../config/dpo/MiniLLM-0.2B-DPO/bert4torch_config.json"  # 预训练模型权重config
  save_dir: "../ckpt/dpo/MiniLLM-0.2B-DPO"  # 模型ckpt保存路径
  dataset_save_dir: "../data/dpo_data"  # dpo的训练数据所在路径
  use_peft: True
  fp16: True

data_process:
  MAX_LENGTH: 1024  # 序列最大长度
  pad_token_id: 0  # pad_token_id
  eos_token_id: 2  # eos_token_id
  dataset_src_dir: "/data/corpus/alignment/"  # dpo语料所在路径
  file_names: [
          "AI-ModelScope@hh_rlhf_cn/hh_rlhf_train.jsonl",
          "AI-ModelScope@hh_rlhf_cn/hh_rlhf_test.jsonl",
          "hiyouga@DPO-En-Zh-20k/dpo_zh.json",
          "iic@CValues-Comparison/train.jsonl",
          "iic@CValues-Comparison/test.jsonl",
          "liyucheng@zhihu_rlhf_3k/zhihu_3k_rlfh.tsv",
          "beyond@rlhf-reward-single-round-trans_chinese/train-00000-of-00001-789dc5dece0f1fc1.parquet",
          "beyond@rlhf-reward-single-round-trans_chinese/test-00000-of-00001-8ecd46436fadcf7f.parquet",
      ]
  dataset_save_dir: "../data/dpo_data"  # 处理好的文件存储路径
  max_samples: null  # None表示不限制，不为None用于测试小样本快速验证
  max_samples_per_file: 100000  # 每个文件最多能容纳的样本量，用于切分大文件
