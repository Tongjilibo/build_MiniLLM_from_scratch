
MAX_LENGTH: 896  # 序列最大长度
pad_token_id: 0  # pad_token_id
eos_token_id: 2  # eos_token_id
dataset_src_dir: "/data/corpus/sft/common/"  # sft语料所在路径
file_names: [
        "alpaca-zh/alpaca_gpt4_data_zh.json",
        "BelleGroup/Belle_open_source_0.5M.json",
        "BelleGroup/Belle_open_source_1M.json",
        "BelleGroup/school_math_0.25M.json",
        "deepctrl-sft-data/sft_data_zh.jsonl",
        "moss-002-sft-data/zh_helpfulness.json",
        "moss-002-sft-data/zh_honesty.json",
        "moss-003-sft-data/moss-003-sft-no-tools.jsonl",
        "CodeChat/continue_zh.jsonl",
        "CodeChat/continue_zh_2.jsonl",
        "ShareGPT-Chinese-English-90k/common_zh_70k.jsonl",
        "ShareGPT-Chinese-English-90k/computer_cn_26k_continue.jsonl",
        "ShareGPT-Chinese-English-90k/computer_zh_26k.jsonl",
        "ShareGPT-Chinese-English-90k/unknow_zh_38k.jsonl",
        "ShareGPT-Chinese-English-90k/unknow_zh_38k_continue.jsonl",
        "firefly-train-1.1M/firefly-train-1.1M.jsonl"
    ]  # sft待处理的数据集，因为数据集很大，按照实际情况按需使用，比如只使用alpaca-zh
dataset_save_dir: "../sft_data"  # 处理好的文件存储路径
max_samples: null  # None表示不限制，不为None用于测试小样本快速验证
max_samples_per_file: 100000  # 每个文件最多能容纳的样本量，用于切分大文件
