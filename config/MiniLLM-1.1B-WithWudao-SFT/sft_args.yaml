one_dataset_every_time: false  # false表示一次把所有sft数据吃进内存，true表示一次只使用一个dataset
lr: 0.00002  # 学习率
batch_size: 8  # 每张卡的batch size
grad_accumulation_steps: 1  # 梯度累计
pad_token_id: 0  # pad token id
max_length: 896   # 序列最大长度
epochs: 5  # 训练的epoch数
weight_decay: 0.1  # 权重衰减
interval: 2000  # 保存权重的步数
torch_dtype: null  # 默认使用混合精度训练，可以制定为torch.float32，torch.float16或者torch.bfloat16
model_path: "../ckpt/MiniLLM-1.1B-WithWudao/final/model.pt"  # 预训练模型权重路径
config_path: "../config/MiniLLM-1.1B-WithWudao-SFT_Alpaca/bert4torch_config.json"  # 预训练模型权重config
save_dir: "../ckpt/MiniLLM-1.1B-WithWudao-SFT_Alpaca"  # 模型ckpt保存路径
dataset_save_dir: "../sft_data"  # sft的训练数据所在路径